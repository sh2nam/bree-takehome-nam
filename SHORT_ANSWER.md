### 1. **Post-treatment bias: A PM asks: “Let’s measure if instant transfer reduces default by comparing users who chose instant vs standard.” Explain why this is biased and propose an experiment or instrument to estimate a causal effect.**

Comparing default rates between users who chose instant vs. standard transfers is biased because the choice itself is post-treatment and reflects underlying traits that also affect repayment. For example, fraudsters or malicious actors are unlikely to pay extra fees since they never intend to repay, while more financially literate customers may proactively use instant to avoid missing bills. Liquidity-constrained borrowers might lean on instant transfers when cash is tight, which is also correlated with higher default risk, whereas higher-income or more trusting customers may adopt instant more freely and default less. Because these differences drive both the choice of transfer type and the likelihood of default, the groups are not comparable. To measure a true causal effect, you’d need to either (1) run a randomized controlled trial where customers are randomly assigned to instant vs. standard defaults, ensuring the groups are balanced, or (2) use an instrumental variable, like randomized fee discounts or nudges, that shifts customers toward instant without being directly related to their repayment risk. An RCT is the most accurate and reliable approach, but in cases where randomization isn’t feasible, an instrumental variable provides a practical alternative.

---

### 2. **Overlapping experiments: Price test and Tip prompt test overlap. Give two statistically valid ways to analyze without pausing tests.**

When working with overlapping experiments, two widely used and statistically valid approaches are factorial modeling and stratification. In factorial modeling, instead of treating the experiments as separate A/B tests, you analyze them as one experiment with multiple factors. For example, a price test (A vs. B) crossed with a tip prompt test (control, persuasive, or social proof) creates a factorial design with all combinations, allowing you to estimate the main effect of price, the main effect of tips, and whether there are interaction effects between them. In stratification, you estimate the price effect separately within each tip variant (and vice versa), then pool the stratum-specific effects to obtain an overall estimate, while also checking for heterogeneity across strata.

---

### 3. **Metric design: Propose a North Star metric for Bree’s first act (short-term credit) and 3 guardrails. Explain trade-offs and the perverse incentives each might create.**

A strong North Star for Bree’s short-term credit product is repayment-adjusted loan volume—the total loan volume scaled by how much is actually repaid—since it balances growth with credit quality. To keep this metric healthy, three guardrails matter. First, track the default rate to make sure growth isn’t coming from loans that don’t get paid back, though pushing this too far could cut off higher-risk but still creditworthy customers. Second, monitor customer retention, since repeat borrowing shows trust in Bree, but if overemphasized it could encourage cycling customers into new loans and create debt traps. Third, watch unit economics—profitability per loan after funding fees and defaults—so that growth is sustainable, but pressure to maximize margins might lead to higher fees or weaker customer support. Together, these guardrails ensure the North Star reflects both customer impact and long-term business health.

---

### 4. **Data contracts: Name 3 column-level assertions you’d enforce on loans and how you’d alert on drifts that break risk models.**

For loans data, three key column-level assertions I’d enforce are on loan amounts, timestamps, and risk scores. Loan amounts must be greater than zero and not exceed the policy cap, which can vary by customer segment. Timestamps such as requested_at, approved_at, disbursed_at, and due_date should all be stored in UTC and follow a logical sequence (requested_at ≤ approved_at ≤ disbursed_at < due_date), with maximum allowable gaps defined (for example, approval to disbursement within 24 hours). Risk inputs or scores like baseline risk score or probability of default must always fall within [0,1] and maintain a very low null rate (e.g., less than 0.5%). To guard against drifts that could break risk models, I would set up monitoring on key input features and model outputs. For inputs such as loan_amount or risk_score, I’d track distribution changes using metrics like the Population Stability Index (PSI), with thresholds (e.g., PSI > 0.1 as a warning, > 0.25 as critical). For model scores, I’d monitor shifts in the score distribution and approval rates compared to training baselines, while for outcomes I’d track rolling AUC or calibration once repayment labels mature. Alerts would be routed to Slack or email for warnings, with sample records attached for investigation. This ensures data quality issues or population shifts are caught early before they degrade model performance.
